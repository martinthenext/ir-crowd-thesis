---
title: "Accuracy Sequences"
output: 
  html_document: 
    keep_md: yes
---

Loading data.

```{r}

filename <- "seq-1to3.filtered.tsv"

col.names <- c("AC", "NVotes", "RunId", "Method", "Topic", "Accuracy")
col.types <- c("character", "numeric", "factor", "factor", "factor", "numeric")

accuracy <- read.delim(filename, header=FALSE, col.names = col.names, colClasses = col.types)
accuracy$AC <- NULL

head(accuracy)
nrow(accuracy)
```

Look at amount of data available for different topics:

```{r results='asis'}
library(knitr)

topic.rows.sorted <- as.data.frame(sort(table(accuracy$Topic)))
colnames(topic.rows.sorted) <- "Points in dataset"
kable(topic.rows.sorted, format="markdown")

```


## Accuracy accross all topics

First, it would be useful to know amount of documents in each topic. We load it from a `csv` file.

```{r}

filename <- "NDocumentsPerTopic.csv"

ndocs <- read.delim(filename, header=TRUE, sep=",", 
                    colClasses = c("factor", "integer")
                    )
```

Now plotting learning curves across topic *alighning them at a constant vote per document rate*.

To this end, let us add another column to the `accuracy` frame, indicating for every number of votes a corresponding (topic-specific) vote per document rate.

```{r}

accuracy$VotesPerDoc <- NA

for (topic in levels(ndocs$Topic)) {
  accuracy.for.topic <- accuracy[accuracy$Topic == topic, ]
  n.docs.for.topic <- ndocs[ndocs$Topic == topic, ]$NDocuments
  votes.per.doc <- accuracy.for.topic$NVotes / n.docs.for.topic
  accuracy[accuracy$Topic == topic, "VotesPerDoc"] <- votes.per.doc
}

head(accuracy)

```

Let us calculate overall mean accuracy for pair (number of votes, method)

```{r}

plot.for.method <- function(means, method.name, color=NULL) {
  means.for.method <- means[means$Method == method.name, ]
  if (is.null(color)) {
    plot(Accuracy ~ VotesPerDoc, 
       data=means.for.method, 
       ylim=c(0.5, 1.0),
       xlab="Votes per document",
       ylab="Accuracy",
       pch=16, cex=0.3
       )
  } else {
    points(Accuracy ~ VotesPerDoc, data=means.for.method, col=color, pch=16, cex=0.1)
  }
}

plot.learning.curves <- function(means) {
  plot.for.method(means, 'MV')
  plot.for.method(means, 'MEV(1)', "red")
  plot.for.method(means, 'MVNN(0.5)', "green")
  plot.for.method(means, 'GP', "blue")
  
  legend(x="bottomright", c("MV", "MEV(1)", "MVNN(0.5)", "GP"), lty=c(1,1,1,1), col=c("black", "red", "green", "blue") )
}

means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy, mean)
plot.learning.curves(means)

  
```

Accuracies are averaged across all topics and color for different methods.

Separately calculating methods means at 1, 2 and 3 votes per document.

```{r}

votes.per.doc <- 1.0
means.vpd <- aggregate(Accuracy ~ Method, accuracy[accuracy$VotesPerDoc==votes.per.doc, ], mean)
colnames(means.vpd) <- c("Method", "AccuracyAt1VPD")

votes.per.doc <- 2.0
means.2vpd <- aggregate(Accuracy ~ Method, accuracy[accuracy$VotesPerDoc==votes.per.doc, ], mean)
means.vpd$AccuracyAt2VPD <- means.2vpd$Accuracy

votes.per.doc <- 3.0
means.3vpd <- aggregate(Accuracy ~ Method, accuracy[accuracy$VotesPerDoc==votes.per.doc, ], mean)
means.vpd$AccuracyAt3VPD <- means.3vpd$Accuracy

(means.vpd)

```

### Origin of lines

**Hypothesis 1** There is 6 lines for every method (easy to distinguish on `GP`) - one line for a topic document count. All the accuracties for a particular method are averaged into these 6 bins.

As long as we calculate the means for (method, votesPerDoc) pairs first, we then only agregate 28 numbers into 6 bins:

```{r} 

# means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy, mean)
# plot.learning.curves(means)

```

The variance is high enough for them to differ this way.

If we plot all accuracies with aggregating them, and color them by topic: as there are not many documents with ground truth for every topic, there is only around 10 accuracy levels and plotting accuracies of methods on top of each other is not instructive.

### To get rid of the lines

We can split Votes per document into `0.05` segments and average accuracy on them.


Let us explore the `GP` performance separately for different topics.

## Inspecting GP accuracy

### GP Accuracy colored by topic

We want to plot Accuracy against VotesPerDoc, indicating every separate topic with a color.

```{r}

gp.accuracy <- accuracy[accuracy$Method=='GP', ]

col.codes <- 1:28
col.idx <- 1

plot(c(1.0, 3.0), range(gp.accuracy$Accuracy), col="white")
for (topic in levels(accuracy$Topic) ) {
  gp.accuracy.for.topic <- gp.accuracy[gp.accuracy$Topic == topic, ]
  topic.means <- aggregate(Accuracy ~ VotesPerDoc, gp.accuracy.for.topic, mean)
  points(topic.means$VotesPerDoc, topic.means$Accuracy, pch=16, cex=0.3, col=col.idx)
  col.idx <- col.idx + 1
}

```

Each colored line represents a separate topic.

No obvious clustering here. Let us now color the same lines by the number of documents in a topic.

### GP accuracy colored by document count in a topic

```{r}

possible.doc.counts <- levels(as.factor(ndocs$NDocuments))
color.code <- c("black", "red", "green", "blue", "yellow", "orange")
colors <- numeric(125)

i <- 1
for (doc.count in possible.doc.counts) {
  colors[as.numeric(doc.count)] <- color.code[i]
  i <- i + 1
}

get.color.code.for.topic <- function(topic) {
  # Look up the doc count
  doc.count <- ndocs[ndocs$Topic==topic, ]$NDocuments
  colors[doc.count]
}

# We want to plot Accuracy ~ VotesPerDoc, coloring points depending in doc count
plot(c(1.0, 3.0), range(accuracy$Accuracy), col="white")
for (topic in levels(accuracy$Topic) ) {
  gp.accuracy.for.topic <- gp.accuracy[gp.accuracy$Topic == topic, ]
  topic.means <- aggregate(Accuracy ~ VotesPerDoc, gp.accuracy.for.topic, mean)
  points(topic.means$VotesPerDoc, topic.means$Accuracy, pch=16, cex=0.3, col=get.color.code.for.topic(topic))
  col.idx <- col.idx + 1
}

```

Lines of the same color are the ones that come from topics with similar document counts. Doesn't look like for GPs a number of documents in a topic influences accuracy.

<!--

## Looking at slices of data

Plot the same thing for 1/3 of the data

```{r}

total.n.rows <- nrow(accuracy)
third <- total.n.rows/3
accuracy.third <- accuracy[1:third, ]
means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.third, mean)
plot.learning.curves(means)

```

Plot the same thing for 2/3 of the data

```{r}

total.n.rows <- nrow(accuracy)
two.third <- 2*total.n.rows/3
accuracy.two.third <- accuracy[1:two.third, ]
means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.two.third, mean)
plot.learning.curves(means)

```

-->

## For each topic separately

```{r}

for (topic in levels(accuracy$Topic) ) {
  cat("Topic: ", topic, "\n")
  accuracy.for.topic <- accuracy[accuracy$Topic == topic, ]
  accuracy.for.topic$RunId <- factor(accuracy.for.topic$RunId)
  cat("Runs plotted: ", length(levels(accuracy.for.topic$RunId)), "\n")
  means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.for.topic, mean)
  plot.learning.curves(means)
}

```
