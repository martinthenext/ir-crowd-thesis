---
title: "Accuracy Sequences"
output: 
  html_document: 
    keep_md: yes
---

Loading data.

```{r}

filename <- "seq-1to3.filtered.tsv"

col.names <- c("AC", "NVotes", "RunId", "Method", "Topic", "Accuracy")
col.types <- c("character", "numeric", "factor", "factor", "factor", "numeric")

accuracy <- read.delim(filename, header=FALSE, col.names = col.names, colClasses = col.types)
accuracy$AC <- NULL

head(accuracy)
nrow(accuracy)
```

Look at amount of data available for different topics:

```{r results='asis'}
library(knitr)

topic.rows.sorted <- as.data.frame(sort(table(accuracy$Topic)))
colnames(topic.rows.sorted) <- "Points in dataset"
kable(topic.rows.sorted, format="markdown")

```


## Accuracy accross all topics

First, it would be useful to know amount of documents in each topic. We load it from a `csv` file.

```{r}

filename <- "NDocumentsPerTopic.csv"

ndocs <- read.delim(filename, header=TRUE, sep=",", 
                    colClasses = c("factor", "integer")
                    )
```

Now plotting learning curves across topic *alighning them at a constant vote per document rate*.

To this end, let us add another column to the `accuracy` frame, indicating for every number of votes a corresponding (topic-specific) vote per document rate.

```{r}

accuracy$VotesPerDoc <- NA

for (topic in levels(ndocs$Topic)) {
  accuracy.for.topic <- accuracy[accuracy$Topic == topic, ]
  n.docs.for.topic <- ndocs[ndocs$Topic == topic, ]$NDocuments
  votes.per.doc <- accuracy.for.topic$NVotes / n.docs.for.topic
  accuracy[accuracy$Topic == topic, "VotesPerDoc"] <- votes.per.doc
}

head(accuracy)

```

Let us calculate overall mean accuracy for pair (number of votes, method)

```{r}

means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy, mean)

plot.for.method <- function(means, method.name, color=NULL) {
  means.for.method <- means[means$Method == method.name, ]
  if (is.null(color)) {
    plot(Accuracy ~ VotesPerDoc, 
       data=means.for.method, 
       ylim=c(0.5, 1.0),
       type="l",
       xlab="Votes per document",
       ylab="Accuracy")    
  } else {
    lines(Accuracy ~ VotesPerDoc, data=means.for.method, col=color)
  }
}

plot.learning.curves <- function(means) {
  plot.for.method(means, 'MV')
  plot.for.method(means, 'MEV(1)', "red")
  plot.for.method(means, 'MVNN(0.5)', "green")
  plot.for.method(means, 'GP', "blue")
  
  legend(x="bottomright", c("MV", "MEV(1)", "MVNN(0.5)", "GP"), lty=c(1,1,1,1), col=c("black", "red", "green", "blue") )
}

plot.learning.curves(means)

```


Variance on the ends of the plot is higher, because for every topic the data is from 1 to 3 votes per document. Number of documents varies from 90 to 125, and there is more data for average number of votes.

## For each topic separately

```{r}

for (topic in levels(accuracy$Topic) ) {
  cat("Topic: ", topic, "\n")
  accuracy.for.topic <- accuracy[accuracy$Topic == topic, ]
  accuracy.for.topic$RunId <- factor(accuracy.for.topic$RunId)
  cat("Runs plotted: ", length(levels(accuracy.for.topic$RunId)), "\n")
  means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.for.topic, mean)
  plot.learning.curves(means)
}

```
