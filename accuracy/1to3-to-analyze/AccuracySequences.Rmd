---
title: "Accuracy Sequences"
output: 
  html_document: 
    keep_md: yes
---

Loading data.

```{r}

filename <- "seq-1to3.filtered.tsv"

col.names <- c("AC", "NVotes", "RunId", "Method", "Topic", "Accuracy")
col.types <- c("character", "numeric", "factor", "factor", "factor", "numeric")

accuracy <- read.delim(filename, header=FALSE, col.names = col.names, colClasses = col.types)
accuracy$AC <- NULL

head(accuracy)
nrow(accuracy)
```

Look at amount of data available for different topics:

```{r results='asis'}
library(knitr)

topic.rows.sorted <- as.data.frame(sort(table(accuracy$Topic)))
colnames(topic.rows.sorted) <- "Points in dataset"
kable(topic.rows.sorted, format="markdown")

```


## Accuracy accross all topics

First, it would be useful to know amount of documents in each topic. We load it from a `csv` file.

```{r}

filename <- "NDocumentsPerTopic.csv"

ndocs <- read.delim(filename, header=TRUE, sep=",", 
                    colClasses = c("factor", "integer")
                    )
```

Now plotting learning curves across topic *alighning them at a constant vote per document rate*.

To this end, let us add another column to the `accuracy` frame, indicating for every number of votes a corresponding (topic-specific) vote per document rate.

```{r}

accuracy$VotesPerDoc <- NA

for (topic in levels(ndocs$Topic)) {
  accuracy.for.topic <- accuracy[accuracy$Topic == topic, ]
  n.docs.for.topic <- ndocs[ndocs$Topic == topic, ]$NDocuments
  votes.per.doc <- accuracy.for.topic$NVotes / n.docs.for.topic
  accuracy[accuracy$Topic == topic, "VotesPerDoc"] <- votes.per.doc
}

head(accuracy)

```

Let us calculate overall mean accuracy for pair (number of votes, method)

```{r}

plot.for.method <- function(means, method.name, color=NULL) {
  means.for.method <- means[means$Method == method.name, ]
  if (is.null(color)) {
    plot(Accuracy ~ VotesPerDoc, 
       data=means.for.method, 
       ylim=c(0.5, 1.0),
       xlab="Votes per document",
       ylab="Accuracy",
       pch='.'
       )
  } else {
    points(Accuracy ~ VotesPerDoc, data=means.for.method, col=color, pch='.')
  }
}

plot.learning.curves <- function(means) {
  plot.for.method(means, 'MV')
  plot.for.method(means, 'MEV(1)', "red")
  plot.for.method(means, 'MVNN(0.5)', "green")
  plot.for.method(means, 'GP', "blue")
  
  legend(x="bottomright", c("MV", "MEV(1)", "MVNN(0.5)", "GP"), lty=c(1,1,1,1), col=c("black", "red", "green", "blue") )
}

means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy, mean)
plot.learning.curves(means)

  
```

Separately calculating methods means at 1, 2 and 3 votes per document.

```{r}

votes.per.doc <- 1.0
means.vpd <- aggregate(Accuracy ~ Method, accuracy[accuracy$VotesPerDoc==votes.per.doc, ], mean)
colnames(means.vpd) <- c("Method", "AccuracyAt1VPD")

votes.per.doc <- 2.0
means.2vpd <- aggregate(Accuracy ~ Method, accuracy[accuracy$VotesPerDoc==votes.per.doc, ], mean)
means.vpd$AccuracyAt2VPD <- means.2vpd$Accuracy

votes.per.doc <- 3.0
means.3vpd <- aggregate(Accuracy ~ Method, accuracy[accuracy$VotesPerDoc==votes.per.doc, ], mean)
means.vpd$AccuracyAt3VPD <- means.3vpd$Accuracy

(means.vpd)

```

We see interesting lines of performance for topics with similar numbers of documents. Let us explore the `GP` performance separately for different topics.

```{r}

gp.accuracy <- accuracy[accuracy$Method=='GP', ]

col.codes <- 1:28
col.idx <- 1

# We want to plot Accuracy ~ VotesPerDoc, coloring points depending in Topic
plot(c(1.0, 3.0), range(accuracy$Accuracy), col="white")
for (topic in levels(accuracy$Topic) ) {
  gp.accuracy.for.topic <- gp.accuracy[gp.accuracy$Topic == topic, ]
  topic.means <- aggregate(Accuracy ~ VotesPerDoc, gp.accuracy.for.topic, mean)
  points(topic.means$VotesPerDoc, topic.means$Accuracy, pch=16, cex=0.3, col=col.idx)
  col.idx <- col.idx + 1
}

```


<!--

## Looking at slices of data

Plot the same thing for 1/3 of the data

```{r}

total.n.rows <- nrow(accuracy)
third <- total.n.rows/3
accuracy.third <- accuracy[1:third, ]
means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.third, mean)
plot.learning.curves(means)

```

Plot the same thing for 2/3 of the data

```{r}

total.n.rows <- nrow(accuracy)
two.third <- 2*total.n.rows/3
accuracy.two.third <- accuracy[1:two.third, ]
means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.two.third, mean)
plot.learning.curves(means)

```

-->

## For each topic separately

```{r}

for (topic in levels(accuracy$Topic) ) {
  cat("Topic: ", topic, "\n")
  accuracy.for.topic <- accuracy[accuracy$Topic == topic, ]
  accuracy.for.topic$RunId <- factor(accuracy.for.topic$RunId)
  cat("Runs plotted: ", length(levels(accuracy.for.topic$RunId)), "\n")
  means <- aggregate(Accuracy ~ Method + VotesPerDoc, accuracy.for.topic, mean)
  plot.learning.curves(means)
}

```
